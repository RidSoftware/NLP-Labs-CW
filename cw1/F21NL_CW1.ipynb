{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 (Enviroment Setup)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Importing the Datasets\n",
        "wikitext (*full* ) - 859955 docs\n",
        "\n",
        "wikitext (*small* ) - 10000 docs"
      ],
      "metadata": {
        "id": "I02HtuGWggmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQaGMPUu5vQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf766ca-8ad3-4a28-900c-95365034b453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-09 20:07:43--  https://www.dropbox.com/scl/fi/ibd4cmixckghx6hhb361c/wikitext-filtered-full.zip?rlkey=q71cebf0k5fvvwhmcntoswzhq&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.70.18, 2620:100:6057:18::a27d:d12\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.70.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uccff996ba72a552ce0d939aa523.dl-eu.dropboxusercontent.com/cd/0/inline/Cy7aPsq_UKgH1asNm1c4qbSLSVf2FTGyLqDSEnoH6r44ymsGIrxIP5scDY_7C6KxGNL-psklCEUNODEH34FCBKD5NlUJfc-o2DAIFwV3OFf0tc57V8Gfu-IKgtFzvqaxsZ9lwrDeGkYSh5b0Gg785pBq/file?dl=1# [following]\n",
            "--2025-10-09 20:07:44--  https://uccff996ba72a552ce0d939aa523.dl-eu.dropboxusercontent.com/cd/0/inline/Cy7aPsq_UKgH1asNm1c4qbSLSVf2FTGyLqDSEnoH6r44ymsGIrxIP5scDY_7C6KxGNL-psklCEUNODEH34FCBKD5NlUJfc-o2DAIFwV3OFf0tc57V8Gfu-IKgtFzvqaxsZ9lwrDeGkYSh5b0Gg785pBq/file?dl=1\n",
            "Resolving uccff996ba72a552ce0d939aa523.dl-eu.dropboxusercontent.com (uccff996ba72a552ce0d939aa523.dl-eu.dropboxusercontent.com)... 162.125.70.15, 2620:100:6028:15::a27d:470f\n",
            "Connecting to uccff996ba72a552ce0d939aa523.dl-eu.dropboxusercontent.com (uccff996ba72a552ce0d939aa523.dl-eu.dropboxusercontent.com)|162.125.70.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Cy7dDAkCN3Iyth5o6Z2pk9rxILzBuARLpId05hTkl3Mpf0K3zTMxWt3Z_hCmVrR2LZ8r4JJ7N0Yuy3ktmU1CKTlkdO8smMU4hkVM6gWsZETekOGwX08FUoV_-nrXz9iUSzru9qQoiFe7DMTsPF5uw2In9VKZvwEaeNvcmW1VKsf_nIl3aPgSjCVdyNffiCuBqCS7XFZ2yGFNgDNGbWWDIPm03TRSBHhcdhk_RNEWtOF5hZeszl-UU94Lponj6MSSo6GWQmg-PVwxcnFfKFJ-0-vRNxItK2kbHKA8LGES1U_LE6IaE5teAXN2tEHcmtJ9gn_TS7agFchz3sGbLlt_oCFxZJkcfaAQnaBM0mPSX9-myRnFNmo_UK-lNh8vS4KQXGg/file?dl=1 [following]\n",
            "--2025-10-09 20:07:46--  https://uccff996ba72a552ce0d939aa523.dl-eu.dropboxusercontent.com/cd/0/inline2/Cy7dDAkCN3Iyth5o6Z2pk9rxILzBuARLpId05hTkl3Mpf0K3zTMxWt3Z_hCmVrR2LZ8r4JJ7N0Yuy3ktmU1CKTlkdO8smMU4hkVM6gWsZETekOGwX08FUoV_-nrXz9iUSzru9qQoiFe7DMTsPF5uw2In9VKZvwEaeNvcmW1VKsf_nIl3aPgSjCVdyNffiCuBqCS7XFZ2yGFNgDNGbWWDIPm03TRSBHhcdhk_RNEWtOF5hZeszl-UU94Lponj6MSSo6GWQmg-PVwxcnFfKFJ-0-vRNxItK2kbHKA8LGES1U_LE6IaE5teAXN2tEHcmtJ9gn_TS7agFchz3sGbLlt_oCFxZJkcfaAQnaBM0mPSX9-myRnFNmo_UK-lNh8vS4KQXGg/file?dl=1\n",
            "Reusing existing connection to uccff996ba72a552ce0d939aa523.dl-eu.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191859755 (183M) [application/binary]\n",
            "Saving to: ‘wikitext-filtered-full.zip’\n",
            "\n",
            "wikitext-filtered-f 100%[===================>] 182.97M  26.6MB/s    in 8.0s    \n",
            "\n",
            "2025-10-09 20:07:54 (23.0 MB/s) - ‘wikitext-filtered-full.zip’ saved [191859755/191859755]\n",
            "\n",
            "--2025-10-09 20:07:54--  https://www.dropbox.com/scl/fi/ek174r3sg7qjx0aa9atop/wikitext-filtered-10k.zip?rlkey=zy6jqxv6qsc16lr9qm3ki9uhf&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6028:18::a27d:4712\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc47c47cf2f8a886c93c221f5e44.dl-eu.dropboxusercontent.com/cd/0/inline/Cy6ZNrMh0RElYQM8eSLZkzAg3x_-2jH_E6ixkOzaPBc-y0p7P3F2EKUlUEQZPdj1MCIwr-1Tmsl8anBlIBdfTQ97PlATChyo8aMJnHhpg1-29EQqebyju56TS7o-a8GjBlyEfjNsKNTTwjh_YUKe48Wx/file?dl=1# [following]\n",
            "--2025-10-09 20:07:55--  https://uc47c47cf2f8a886c93c221f5e44.dl-eu.dropboxusercontent.com/cd/0/inline/Cy6ZNrMh0RElYQM8eSLZkzAg3x_-2jH_E6ixkOzaPBc-y0p7P3F2EKUlUEQZPdj1MCIwr-1Tmsl8anBlIBdfTQ97PlATChyo8aMJnHhpg1-29EQqebyju56TS7o-a8GjBlyEfjNsKNTTwjh_YUKe48Wx/file?dl=1\n",
            "Resolving uc47c47cf2f8a886c93c221f5e44.dl-eu.dropboxusercontent.com (uc47c47cf2f8a886c93c221f5e44.dl-eu.dropboxusercontent.com)... 162.125.70.15, 2620:100:6057:15::a27d:d0f\n",
            "Connecting to uc47c47cf2f8a886c93c221f5e44.dl-eu.dropboxusercontent.com (uc47c47cf2f8a886c93c221f5e44.dl-eu.dropboxusercontent.com)|162.125.70.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Cy5WM04MbzadRRDyzLjye293ZWjSvH1Y9NQJZOmVO_pmg9cFinBlp8PhsFqFdqKNjFeBZ_SsXsT9c7_bU46BhdMXsrUsO3pdM2-BObqU7wgwsn18J_Cvsu9vC6B9kPj6ySfsO1szPmQAA3G15uZF1ihZ3CB0lHTEzSyRX08XEI2mQHd3mm3vjCUcH2WVvWJl0T4h2I4ICarEvW8vwVs8tqQ52cftwehJoe9GfSc8gNt_dWa7bdVDezT1bfNBXZFXQRgpwUB6wy3gVVR47EoR2AH6rekUsR5zXp-N4t_zFfSMU8U4Ce0RQYfIHbXKgaa-2bCg_ZCntuKmuHWoSJqeS4j6Gl10CWOtSO_atwhrGHfgcNDSYlkFpzbKTbTzNL4Ye7s/file?dl=1 [following]\n",
            "--2025-10-09 20:07:56--  https://uc47c47cf2f8a886c93c221f5e44.dl-eu.dropboxusercontent.com/cd/0/inline2/Cy5WM04MbzadRRDyzLjye293ZWjSvH1Y9NQJZOmVO_pmg9cFinBlp8PhsFqFdqKNjFeBZ_SsXsT9c7_bU46BhdMXsrUsO3pdM2-BObqU7wgwsn18J_Cvsu9vC6B9kPj6ySfsO1szPmQAA3G15uZF1ihZ3CB0lHTEzSyRX08XEI2mQHd3mm3vjCUcH2WVvWJl0T4h2I4ICarEvW8vwVs8tqQ52cftwehJoe9GfSc8gNt_dWa7bdVDezT1bfNBXZFXQRgpwUB6wy3gVVR47EoR2AH6rekUsR5zXp-N4t_zFfSMU8U4Ce0RQYfIHbXKgaa-2bCg_ZCntuKmuHWoSJqeS4j6Gl10CWOtSO_atwhrGHfgcNDSYlkFpzbKTbTzNL4Ye7s/file?dl=1\n",
            "Reusing existing connection to uc47c47cf2f8a886c93c221f5e44.dl-eu.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2230160 (2.1M) [application/binary]\n",
            "Saving to: ‘wikitext-filtered-10k.zip’\n",
            "\n",
            "wikitext-filtered-1 100%[===================>]   2.13M  1.99MB/s    in 1.1s    \n",
            "\n",
            "2025-10-09 20:07:58 (1.99 MB/s) - ‘wikitext-filtered-10k.zip’ saved [2230160/2230160]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O wikitext-filtered-full.zip \"https://www.dropbox.com/scl/fi/ibd4cmixckghx6hhb361c/wikitext-filtered-full.zip?rlkey=q71cebf0k5fvvwhmcntoswzhq&dl=1\"\n",
        "!wget -O wikitext-filtered-10k.zip \"https://www.dropbox.com/scl/fi/ek174r3sg7qjx0aa9atop/wikitext-filtered-10k.zip?rlkey=zy6jqxv6qsc16lr9qm3ki9uhf&dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GgyhgwO59KU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "408eab3c-b80b-4d52-efd3-620e80662b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  wikitext-filtered-full.zip\n",
            "   creating: wikitext-filtered-full/\n",
            "  inflating: wikitext-filtered-full/dataset_info.json  \n",
            "  inflating: wikitext-filtered-full/state.json  \n",
            "  inflating: wikitext-filtered-full/data-00000-of-00001.arrow  \n",
            "Archive:  wikitext-filtered-10k.zip\n",
            "   creating: wikitext-filtered-10k/\n",
            "  inflating: wikitext-filtered-10k/dataset_info.json  \n",
            "  inflating: wikitext-filtered-10k/state.json  \n",
            "  inflating: wikitext-filtered-10k/data-00000-of-00001.arrow  \n"
          ]
        }
      ],
      "source": [
        "!unzip wikitext-filtered-full.zip\n",
        "!unzip wikitext-filtered-10k.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbUoBOOH5_Zy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d511fc6f-67aa-4914-942e-0bbb2a279fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# datasets package provides dataset tools from hugginface\n",
        "!pip install datasets\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aYe6Rq5k6Bgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c925d93-c747-40a4-a08f-4ef912c0c810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wikitext_small: 10000 docs, wikitext_large: 859955 docs\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "def load_dataset():\n",
        "  wikitext_small = \"wikitext-filtered-10k\"\n",
        "  wikitext_large = \"wikitext-filtered-full\"\n",
        "\n",
        "  dataset_small = Dataset.load_from_disk(wikitext_small)\n",
        "  dataset_large = Dataset.load_from_disk(wikitext_large)\n",
        "  print(\"wikitext_small: {} docs, wikitext_large: {} docs\".format(len(dataset_small), len(dataset_large)))\n",
        "  return dataset_small, dataset_large\n",
        "\n",
        "wikitext_small, wikitext_large = load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Dataset\n",
        "Summary statistics"
      ],
      "metadata": {
        "id": "gAUcAjXfgIFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wt = wikitext_small\n",
        "#wt = wikitext_large\n",
        "\n",
        "print('# TYPE OF THE DATASET:', '\\n', type(wt))\n",
        "print(wt, '\\n')\n",
        "print('# ENTRIES LOOK LIKE:')\n",
        "print(wt.features, '\\n', wt[0], '\\n', wt[1], '\\n')\n",
        "\n",
        "print('# DATASET STATISTICS:')\n",
        "print('No. of docs:', len(wt))\n",
        "lengths = [len(doc['text'].split()) for doc in wt]\n",
        "print('Mean doc length:', sum(lengths)/len(lengths), 'words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di-0S5z9d9NE",
        "outputId": "22cb1543-6fd0-411b-9140-2670d50a2670"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# TYPE OF THE DATASET: \n",
            " <class 'datasets.arrow_dataset.Dataset'>\n",
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 10000\n",
            "}) \n",
            "\n",
            "# ENTRIES LOOK LIKE:\n",
            "{'text': Value('string')} \n",
            " {'text': 'Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" .'} \n",
            " {'text': \"The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n .\"} \n",
            "\n",
            "# DATASET STATISTICS:\n",
            "No. of docs: 10000\n",
            "Mean doc length: 115.0954 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 (Train Baselines)\n",
        "\n",
        "---\n",
        "Installing dependancies\n",
        "- gensim - word2vec models\n",
        "- nltk (natural language tool kit) - stopwords removal"
      ],
      "metadata": {
        "id": "fUWa02Kzq3YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk"
      ],
      "metadata": {
        "id": "5490qFGSwT7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_dataset(dataset):\n",
        "    text_col = 'text' if 'text' in dataset.column_names else dataset.column_names[0]\n",
        "    tokenized = []\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        text = dataset[i][text_col]\n",
        "        if not isinstance(text, str):\n",
        "            continue\n",
        "        tokens = [t.lower() for t in text.split() if t.isalpha() and t.lower() not in stop_words]\n",
        "        if tokens:\n",
        "            tokenized.append(tokens)\n",
        "\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "ZMphBXL8nRVE",
        "outputId": "42fbc6f4-0d62-4cc5-c914-b83de4a9f98f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_small = preprocess_dataset(wikitext_small)\n",
        "tokens_large = preprocess_dataset(wikitext_large)"
      ],
      "metadata": {
        "id": "f8tNPp7Jwb-i"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_word2vec(tokens, model_name, vector_size=50, window=5, min_count=5, epochs=5):\n",
        "    print(f\"Training {model_name} ...\")\n",
        "    model = Word2Vec(\n",
        "        sentences=tokens,\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        epochs=epochs\n",
        "    )\n",
        "    model.save(f\"{model_name}.model\")\n",
        "    model.wv.save(f\"{model_name}.kv\")\n",
        "    print(f\"{model_name} saved.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "vF6w1eeL6xJW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_small = train_word2vec(tokens_small, \"word2vec_small\")\n",
        "model_large = train_word2vec(tokens_large, \"word2vec_large\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMgpuHkuwn0E",
        "outputId": "ce141dc9-373b-44e3-a435-0a0ce02cd897"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training word2vec_small ...\n",
            "word2vec_small saved.\n",
            "Training word2vec_large ...\n",
            "word2vec_large saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# writing model to disk\n",
        "model_small.save(\"word2vec_small.model\")\n",
        "model_large.save(\"word2vec_large.model\")"
      ],
      "metadata": {
        "id": "k5bum-3IM5sA"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_small = Word2Vec.load(\"word2vec_small.model\")\n",
        "model_large = Word2Vec.load(\"word2vec_large.model\")"
      ],
      "metadata": {
        "id": "72LuHLrAN0h_"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(model_small.wv)\n",
        "print(f\"Vocab size (learned by model):\", vocab_size, '\\n')\n",
        "\n",
        "example_tokens = ['plane', 'car', 'planet', 'nurse', 'city', 'country']\n",
        "for token in example_tokens:\n",
        "    if token in model_small.wv:\n",
        "        print(f\"Top-10 similar to '{token}':\", model_small.wv.most_similar(token, topn=10))\n",
        "    else:\n",
        "        print(f\"'{token}' not in vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H9TVBpxxGTo",
        "outputId": "b757f037-2177-492e-d789-d1e1eb98b544"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size (learned by model): 162898 \n",
            "\n",
            "Top-10 similar to 'plane': [('airplane', 0.7798449397087097), ('planes', 0.7587494850158691), ('helicopter', 0.7551898956298828), ('flight', 0.7514647245407104), ('wreckage', 0.7437148690223694), ('airliner', 0.7287061214447021), ('takeoff', 0.7201843857765198), ('saucer', 0.7154824733734131), ('cockpit', 0.7090662717819214), ('amana', 0.701010525226593)]\n",
            "Top-10 similar to 'car': [('cars', 0.8536964058876038), ('truck', 0.8398393988609314), ('motorcycle', 0.8149896860122681), ('lorry', 0.801920473575592), ('suv', 0.7947259545326233), ('vehicle', 0.7883414030075073), ('driver', 0.7828571200370789), ('cab', 0.7772312164306641), ('taxi', 0.7629276514053345), ('tires', 0.7254185080528259)]\n",
            "Top-10 similar to 'planet': [('earth', 0.8504156470298767), ('galaxy', 0.835283100605011), ('pluto', 0.7836962938308716), ('planets', 0.7812984585762024), ('asteroid', 0.7718139886856079), ('uranus', 0.7695909738540649), ('universe', 0.7661747932434082), ('jupiter', 0.7612099051475525), ('neptune', 0.7575763463973999), ('supernova', 0.7567441463470459)]\n",
            "Top-10 similar to 'nurse': [('surgeon', 0.7863967418670654), ('matron', 0.7805107235908508), ('nanny', 0.7734796404838562), ('therapist', 0.7701335549354553), ('intern', 0.7607481479644775), ('housekeeper', 0.7600995898246765), ('schwester', 0.753458559513092), ('prostitute', 0.744369626045227), ('dentist', 0.7410475611686707), ('midwife', 0.7406240105628967)]\n",
            "Top-10 similar to 'city': [('town', 0.7932745814323425), ('suburbs', 0.7008221745491028), ('cities', 0.6958555579185486), ('downtown', 0.6860154271125793), ('minneapolis', 0.6775717735290527), ('towns', 0.6729977130889893), ('neighborhood', 0.6519626379013062), ('waterfront', 0.6501344442367554), ('metropolitan', 0.6485058665275574), ('capital', 0.6436895728111267)]\n",
            "Top-10 similar to 'country': [('nation', 0.7754936218261719), ('world', 0.6323301792144775), ('europe', 0.6227652430534363), ('countries', 0.6060277223587036), ('european', 0.5809457898139954), ('india', 0.5773490071296692), ('africans', 0.5741065144538879), ('britain', 0.5740565657615662), ('blacks', 0.569057285785675), ('state', 0.562152087688446)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "tPze_vBWUket"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "-AvWc8V9GKDj"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mag(v):\n",
        "  s = sum((e*e) for e in v)\n",
        "  s = math.sqrt(s)\n",
        "  return s\n",
        "\n",
        "def cosineSimilarity(v1, v2):\n",
        "  dotProd = np.dot(v1, v2)\n",
        "  cos = dotProd/(mag(v1)*mag(v2))\n",
        "  return cos"
      ],
      "metadata": {
        "id": "_VJaT2I9_Ren"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "planeIndex = model_small.wv.key_to_index['plane']\n",
        "carIndex = model_small.wv.key_to_index['car']\n",
        "planetIndex = model_small.wv.key_to_index['planet']\n",
        "sunIndex = model_small.wv.key_to_index['sun']\n",
        "passengerIndex = model_small.wv.key_to_index['passenger']"
      ],
      "metadata": {
        "id": "JDg-ByjIGc0I"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v1 = model_large.wv['planet']\n",
        "v2 = model_large.wv['sun']\n",
        "\n",
        "cosineSimilarity(v1, v2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--jP1qvpGNme",
        "outputId": "c0b78992-1d92-4e1b-98b7-4952af65d080"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6306870286899026"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4"
      ],
      "metadata": {
        "id": "e8MAgY7DUS80"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ZiC8uMRUScW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "athnlp-lab3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}