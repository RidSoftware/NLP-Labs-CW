{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 (Enviroment Setup)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Importing the Datasets\n",
        "wikitext (*full* ) - 859955 docs\n",
        "\n",
        "wikitext (*small* ) - 10000 docs"
      ],
      "metadata": {
        "id": "I02HtuGWggmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQaGMPUu5vQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30aa073d-8d6f-4a80-cecd-75a5ff321ba4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-17 23:04:24--  https://www.dropbox.com/scl/fi/ibd4cmixckghx6hhb361c/wikitext-filtered-full.zip?rlkey=q71cebf0k5fvvwhmcntoswzhq&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:601c:18::a27d:612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com/cd/0/inline/CzaM-GcUFl25wnxZLuuZEjPle2s8cbOBXGUdauuTx4bVyCkbHbKJUfztLG3QrW5FRDVIUwJSHM88MN9fg7ti5opxoNh6rR4SszpW_0stmIH7BuoPSPh_MbPtg1K0JHjBqlKuDbSk_AEyCcpoIy8yHpzC/file?dl=1# [following]\n",
            "--2025-10-17 23:04:25--  https://uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com/cd/0/inline/CzaM-GcUFl25wnxZLuuZEjPle2s8cbOBXGUdauuTx4bVyCkbHbKJUfztLG3QrW5FRDVIUwJSHM88MN9fg7ti5opxoNh6rR4SszpW_0stmIH7BuoPSPh_MbPtg1K0JHjBqlKuDbSk_AEyCcpoIy8yHpzC/file?dl=1\n",
            "Resolving uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com (uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com)... 162.125.4.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com (uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com)|162.125.4.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CzatVmYTxzr1lHUtVLUl8t3YmJ_QgdqUpsnLuYJzaEu8Y7CJKowOgXgkHR2Uij936jWMPoS9zA_SiUgAhfuEbrBtxftsQs-izEu1DZH1llm4KT6R5-Z-M42z9CTHO6TWTsAOkkv9Rzjkm7qe_oKxkkhlA4hQtAeUD9eXe25mytE9iAuW7QCbwxsUYDzPqplxrjRzYA9klgmZeDWOz2bWS81G3uyPx6fnOxgAMww4cFsdtcyoa3ul6U5hcOWRPObvcAF_9Tx3Ab8qMVpERF13zXch326tm61AgheKoR30nH0GTziLMcX0gnFCpRDddA1khTTdZa1H8glrTdDXNxw2ouudRs_6oJ6ga3TKHDMD5cFbc0vmIdTDkEE4wyx4tcGNc6o/file?dl=1 [following]\n",
            "--2025-10-17 23:04:26--  https://uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com/cd/0/inline2/CzatVmYTxzr1lHUtVLUl8t3YmJ_QgdqUpsnLuYJzaEu8Y7CJKowOgXgkHR2Uij936jWMPoS9zA_SiUgAhfuEbrBtxftsQs-izEu1DZH1llm4KT6R5-Z-M42z9CTHO6TWTsAOkkv9Rzjkm7qe_oKxkkhlA4hQtAeUD9eXe25mytE9iAuW7QCbwxsUYDzPqplxrjRzYA9klgmZeDWOz2bWS81G3uyPx6fnOxgAMww4cFsdtcyoa3ul6U5hcOWRPObvcAF_9Tx3Ab8qMVpERF13zXch326tm61AgheKoR30nH0GTziLMcX0gnFCpRDddA1khTTdZa1H8glrTdDXNxw2ouudRs_6oJ6ga3TKHDMD5cFbc0vmIdTDkEE4wyx4tcGNc6o/file?dl=1\n",
            "Reusing existing connection to uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191859755 (183M) [application/binary]\n",
            "Saving to: ‘wikitext-filtered-full.zip’\n",
            "\n",
            "wikitext-filtered-f 100%[===================>] 182.97M  35.6MB/s    in 5.9s    \n",
            "\n",
            "2025-10-17 23:04:33 (31.2 MB/s) - ‘wikitext-filtered-full.zip’ saved [191859755/191859755]\n",
            "\n",
            "--2025-10-17 23:04:33--  https://www.dropbox.com/scl/fi/ek174r3sg7qjx0aa9atop/wikitext-filtered-10k.zip?rlkey=zy6jqxv6qsc16lr9qm3ki9uhf&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:601c:18::a27d:612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com/cd/0/inline/CzYAB524zKhymHaDklfBe-FLMMv_dS40O34JQIV9NZ7QjEiEgOKYc8qsXvuBD-aAc-FBysPShI3smpoX3eGr-zZajiktWftS5UI6AcxKvOuOh2yDjvzw3H7an6gm-eeLGGTA_uJuC2Sa5WjKft2Z_dP1/file?dl=1# [following]\n",
            "--2025-10-17 23:04:34--  https://uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com/cd/0/inline/CzYAB524zKhymHaDklfBe-FLMMv_dS40O34JQIV9NZ7QjEiEgOKYc8qsXvuBD-aAc-FBysPShI3smpoX3eGr-zZajiktWftS5UI6AcxKvOuOh2yDjvzw3H7an6gm-eeLGGTA_uJuC2Sa5WjKft2Z_dP1/file?dl=1\n",
            "Resolving uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com (uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com)... 162.125.3.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com (uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CzaS5GNzConvHg-wNOYT7cIC8k0MkKqhVQmCDfwFuyzcXA9XiUez0aUuvE4FZiq5PIItku6u-NHK_xx04JtsgWUPSEMLI96NDLGXVYiR3_HCzxgM82wjfvcfL3_fG9gdi8IfmIEeSHcT94I6OLAB5ohB1gmiKRDuRp5Y82C1F-XkPmwG0OftHZ7H7K4OEhA0ojN9EFd1ofbty2sP3ZxgiLsXAwcf2AdgsBStlELtsvkrOkfx1Exwcx695fS9DmgwjV_cXI_gz-0uPnJKMIk5SS3zUU3DdvhWC9HsOFOyKip0LcjuTtPwvYKFcIllb0oM0211p9rni8DLUPav9827XCwlyZp6ZgJBZVW2dWIONxO-P050sX2WotqpwE2Hcspa_b0/file?dl=1 [following]\n",
            "--2025-10-17 23:04:34--  https://uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com/cd/0/inline2/CzaS5GNzConvHg-wNOYT7cIC8k0MkKqhVQmCDfwFuyzcXA9XiUez0aUuvE4FZiq5PIItku6u-NHK_xx04JtsgWUPSEMLI96NDLGXVYiR3_HCzxgM82wjfvcfL3_fG9gdi8IfmIEeSHcT94I6OLAB5ohB1gmiKRDuRp5Y82C1F-XkPmwG0OftHZ7H7K4OEhA0ojN9EFd1ofbty2sP3ZxgiLsXAwcf2AdgsBStlELtsvkrOkfx1Exwcx695fS9DmgwjV_cXI_gz-0uPnJKMIk5SS3zUU3DdvhWC9HsOFOyKip0LcjuTtPwvYKFcIllb0oM0211p9rni8DLUPav9827XCwlyZp6ZgJBZVW2dWIONxO-P050sX2WotqpwE2Hcspa_b0/file?dl=1\n",
            "Reusing existing connection to uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2230160 (2.1M) [application/binary]\n",
            "Saving to: ‘wikitext-filtered-10k.zip’\n",
            "\n",
            "wikitext-filtered-1 100%[===================>]   2.13M  2.62MB/s    in 0.8s    \n",
            "\n",
            "2025-10-17 23:04:36 (2.62 MB/s) - ‘wikitext-filtered-10k.zip’ saved [2230160/2230160]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O wikitext-filtered-full.zip \"https://www.dropbox.com/scl/fi/ibd4cmixckghx6hhb361c/wikitext-filtered-full.zip?rlkey=q71cebf0k5fvvwhmcntoswzhq&dl=1\"\n",
        "!wget -O wikitext-filtered-10k.zip \"https://www.dropbox.com/scl/fi/ek174r3sg7qjx0aa9atop/wikitext-filtered-10k.zip?rlkey=zy6jqxv6qsc16lr9qm3ki9uhf&dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GgyhgwO59KU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264bd5cd-f122-4120-acfc-fe172a3ad732",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  wikitext-filtered-full.zip\n",
            "replace wikitext-filtered-full/dataset_info.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  wikitext-filtered-10k.zip\n",
            "replace wikitext-filtered-10k/dataset_info.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip wikitext-filtered-full.zip\n",
        "!unzip wikitext-filtered-10k.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbUoBOOH5_Zy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0ee1bc-2b37-46ee-ef5f-4387be6a0f13",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# datasets package provides dataset tools from hugginface\n",
        "!pip install datasets\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYe6Rq5k6Bgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc791372-8bad-41d4-9c6d-3debdf2fccd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wikitext_small: 10000 docs, wikitext_large: 859955 docs\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "def load_dataset():\n",
        "  wikitext_small = \"wikitext-filtered-10k\"\n",
        "  wikitext_large = \"wikitext-filtered-full\"\n",
        "\n",
        "  dataset_small = Dataset.load_from_disk(wikitext_small)\n",
        "  dataset_large = Dataset.load_from_disk(wikitext_large)\n",
        "  print(\"wikitext_small: {} docs, wikitext_large: {} docs\".format(len(dataset_small), len(dataset_large)))\n",
        "  return dataset_small, dataset_large\n",
        "\n",
        "wikitext_small, wikitext_large = load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Dataset\n",
        "Summary statistics"
      ],
      "metadata": {
        "id": "gAUcAjXfgIFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wt = wikitext_small\n",
        "#wt = wikitext_large\n",
        "\n",
        "print('# TYPE OF THE DATASET:', '\\n', type(wt))\n",
        "print(wt, '\\n')\n",
        "print('# ENTRIES LOOK LIKE:')\n",
        "print(wt.features, '\\n', wt[0], '\\n', wt[1], '\\n')\n",
        "\n",
        "print('# DATASET STATISTICS:')\n",
        "print('No. of docs:', len(wt))\n",
        "lengths = [len(doc['text'].split()) for doc in wt]\n",
        "print('Mean doc length:', sum(lengths)/len(lengths), 'words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di-0S5z9d9NE",
        "outputId": "568031af-214b-4ce3-adc0-ff14d197cb68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# TYPE OF THE DATASET: \n",
            " <class 'datasets.arrow_dataset.Dataset'>\n",
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 10000\n",
            "}) \n",
            "\n",
            "# ENTRIES LOOK LIKE:\n",
            "{'text': Value('string')} \n",
            " {'text': 'Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" .'} \n",
            " {'text': \"The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n .\"} \n",
            "\n",
            "# DATASET STATISTICS:\n",
            "No. of docs: 10000\n",
            "Mean doc length: 115.0954 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 (Train Baselines)\n",
        "\n",
        "---\n",
        "Installing dependancies\n",
        "- gensim - word2vec models\n",
        "- nltk (natural language tool kit) - stopwords removal"
      ],
      "metadata": {
        "id": "fUWa02Kzq3YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk"
      ],
      "metadata": {
        "id": "5490qFGSwT7c",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7b6ce2-72d6-41eb-c3f6-97780cf066be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_dataset(dataset):\n",
        "    text_col = 'text' if 'text' in dataset.column_names else dataset.column_names[0]\n",
        "    tokenized = []\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        text = dataset[i][text_col]\n",
        "        if not isinstance(text, str):\n",
        "            continue\n",
        "        tokens = [t.lower() for t in text.split() if t.isalpha() and t.lower() not in stop_words]\n",
        "        if tokens:\n",
        "            tokenized.append(tokens)\n",
        "\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "ZMphBXL8nRVE",
        "outputId": "c04cbeca-0eb1-40d8-c29f-9572d17230bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_small = preprocess_dataset(wikitext_small)\n",
        "tokens_large = preprocess_dataset(wikitext_large)"
      ],
      "metadata": {
        "id": "f8tNPp7Jwb-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_word2vec(tokens, model_name, vector_size=50, window=5, min_count=5, epochs=5):\n",
        "    print(f\"Training {model_name} ...\")\n",
        "    model = Word2Vec(\n",
        "        sentences=tokens,\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        epochs=epochs\n",
        "    )\n",
        "    model.save(f\"{model_name}.model\")\n",
        "    model.wv.save(f\"{model_name}.kv\")\n",
        "    print(f\"{model_name} saved.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "vF6w1eeL6xJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_small = train_word2vec(tokens_small, \"word2vec_small\")\n",
        "model_large = train_word2vec(tokens_large, \"word2vec_large\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMgpuHkuwn0E",
        "outputId": "ba0a5e33-c9cf-4b8c-fe87-1481e105dc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training word2vec_small ...\n",
            "word2vec_small saved.\n",
            "Training word2vec_large ...\n",
            "word2vec_large saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# writing model to disk\n",
        "model_small.save(\"word2vec_small.model\")\n",
        "model_large.save(\"word2vec_large.model\")"
      ],
      "metadata": {
        "id": "k5bum-3IM5sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_small = Word2Vec.load(\"word2vec_small.model\")\n",
        "model_large = Word2Vec.load(\"word2vec_large.model\")"
      ],
      "metadata": {
        "id": "72LuHLrAN0h_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(model_small.wv)\n",
        "print(f\"Vocab size (learned by model):\", vocab_size, '\\n')\n",
        "\n",
        "example_tokens = ['plane', 'car', 'planet', 'nurse', 'city', 'country']\n",
        "for token in example_tokens:\n",
        "    if token in model_small.wv:\n",
        "        print(f\"Top-10 similar to '{token}':\", model_small.wv.most_similar(token, topn=10))\n",
        "    else:\n",
        "        print(f\"'{token}' not in vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H9TVBpxxGTo",
        "outputId": "fba10812-a97c-41ba-c80c-d3d1fabfaf06"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size (learned by model): 13838 \n",
            "\n",
            "Top-10 similar to 'plane': [('channel', 0.9976177215576172), ('probe', 0.997555136680603), ('threatened', 0.9975177049636841), ('briefly', 0.9974639415740967), ('recovery', 0.9973675608634949), ('completion', 0.9973640441894531), ('disaster', 0.997336208820343), ('immediate', 0.9971866011619568), ('repairs', 0.9970893859863281), ('arrive', 0.9970740675926208)]\n",
            "Top-10 similar to 'car': [('bring', 0.9986448287963867), ('demon', 0.9985685348510742), ('becomes', 0.9983323812484741), ('possibility', 0.9982467889785767), ('view', 0.9980642795562744), ('reality', 0.9980185627937317), ('leaves', 0.9980082511901855), ('direct', 0.997999906539917), ('fully', 0.9979443550109863), ('owen', 0.9979330897331238)]\n",
            "Top-10 similar to 'planet': [('slightly', 0.9979651570320129), ('indicated', 0.9977326393127441), ('eye', 0.9974536299705505), ('classification', 0.9972626566886902), ('capable', 0.997245728969574), ('engines', 0.9970810413360596), ('offers', 0.9970569610595703), ('observed', 0.9970449805259705), ('moons', 0.9970031976699829), ('approached', 0.9969111084938049)]\n",
            "Top-10 similar to 'nurse': [('tawny', 0.9985595941543579), ('shark', 0.9983565211296082), ('attributed', 0.9981611967086792), ('effect', 0.9980685710906982), ('green', 0.9979763627052307), ('oxygen', 0.9979245662689209), ('painting', 0.9978524446487427), ('ring', 0.9978417754173279), ('vulnerable', 0.997806191444397), ('stela', 0.997787356376648)]\n",
            "Top-10 similar to 'city': [('state', 0.920772910118103), ('county', 0.9116392731666565), ('national', 0.9073209762573242), ('town', 0.8833724856376648), ('southern', 0.8818227648735046), ('moved', 0.8810088634490967), ('highway', 0.8779670000076294), ('northern', 0.8759586215019226), ('route', 0.8752599954605103), ('headquartered', 0.874614953994751)]\n",
            "Top-10 similar to 'country': [('countries', 0.9848932027816772), ('belgium', 0.9840196371078491), ('usa', 0.9834767580032349), ('network', 0.9832971096038818), ('toured', 0.9821997284889221), ('official', 0.982041597366333), ('riaa', 0.9810083508491516), ('adult', 0.980902373790741), ('industry', 0.9804458022117615), ('shipments', 0.9797428846359253)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "tPze_vBWUket"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "-AvWc8V9GKDj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mag(v):\n",
        "  s = sum((e*e) for e in v)\n",
        "  s = math.sqrt(s)\n",
        "  return s\n",
        "\n",
        "def cosineSimilarity(v1, v2):\n",
        "  dotProd = np.dot(v1, v2)\n",
        "  cos = dotProd/(mag(v1)*mag(v2))\n",
        "  return cos"
      ],
      "metadata": {
        "id": "_VJaT2I9_Ren"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "planeIndex = model_small.wv.key_to_index['plane']\n",
        "carIndex = model_small.wv.key_to_index['car']\n",
        "planetIndex = model_small.wv.key_to_index['planet']\n",
        "sunIndex = model_small.wv.key_to_index['sun']\n",
        "passengerIndex = model_small.wv.key_to_index['passenger']"
      ],
      "metadata": {
        "id": "JDg-ByjIGc0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v1 = model_large.wv['planet']\n",
        "v2 = model_large.wv['sun']\n",
        "\n",
        "cosineSimilarity(v1, v2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--jP1qvpGNme",
        "outputId": "15cad74d-7943-4b04-a205-31f58ce7c06f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6430315907730422"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4"
      ],
      "metadata": {
        "id": "e8MAgY7DUS80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O wordsim353.zip \"www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip\"\n",
        "!unzip wordsim353.zip -d wordsim353"
      ],
      "metadata": {
        "id": "0ZiC8uMRUScW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36949f28-9f46-47c5-c2f2-c330da0f7a4e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-17 23:52:10--  http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
            "Resolving www.gabrilovich.com (www.gabrilovich.com)... 173.236.137.139\n",
            "Connecting to www.gabrilovich.com (www.gabrilovich.com)|173.236.137.139|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://gabrilovich.com/resources/data/wordsim353/wordsim353.zip [following]\n",
            "--2025-10-17 23:52:10--  https://gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
            "Resolving gabrilovich.com (gabrilovich.com)... 173.236.137.139\n",
            "Connecting to gabrilovich.com (gabrilovich.com)|173.236.137.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23257 (23K) [application/zip]\n",
            "Saving to: ‘wordsim353.zip’\n",
            "\n",
            "wordsim353.zip      100%[===================>]  22.71K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-10-17 23:52:10 (842 KB/s) - ‘wordsim353.zip’ saved [23257/23257]\n",
            "\n",
            "Archive:  wordsim353.zip\n",
            "replace wordsim353/combined.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: wordsim353/combined.csv  \n",
            "replace wordsim353/set1.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: wordsim353/set1.csv     \n",
            "replace wordsim353/set2.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: wordsim353/set2.csv     \n",
            "replace wordsim353/combined.tab? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: wordsim353/combined.tab  \n",
            "replace wordsim353/set1.tab? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: wordsim353/set1.tab     \n",
            "replace wordsim353/set2.tab? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: wordsim353/set2.tab     \n",
            "replace wordsim353/instructions.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: wordsim353/instructions.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import spearmanr"
      ],
      "metadata": {
        "id": "00CYbfprfOOd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O combined.zip \"https://raw.githubusercontent.com/kavgan/nlp-text-mining-working-examples/master/wordSimilarity/resources/wordsim353/combined.csv\"\n",
        "\n",
        "!unzip -o combined.zip\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"wordsim353/combined.csv\")  # columns: 'Word 1', 'Word 2', 'Human (mean)'\n",
        "print(\"Loaded: wordsim353/combined.csv | shape:\", df.shape)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03qp2mamfgDI",
        "outputId": "cc0bb848-935a-47fe-f5c8-754644e04284"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  combined.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of combined.zip or\n",
            "        combined.zip.zip, and cannot find combined.zip.ZIP, period.\n",
            "Loaded: wordsim353/combined.csv | shape: (353, 3)\n",
            "     Word 1    Word 2  Human (mean)\n",
            "0      love       sex          6.77\n",
            "1     tiger       cat          7.35\n",
            "2     tiger     tiger         10.00\n",
            "3      book     paper          7.46\n",
            "4  computer  keyboard          7.62\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "athnlp-lab3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}