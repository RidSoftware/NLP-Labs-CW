{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 (Enviroment Setup)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Importing the Datasets\n",
        "wikitext (*full* ) - 859955 docs\n",
        "\n",
        "wikitext (*small* ) - 10000 docs"
      ],
      "metadata": {
        "id": "I02HtuGWggmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQaGMPUu5vQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5223f9e-a93f-4f86-d089-c6f5ba89b6ee",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-18 00:19:13--  https://www.dropbox.com/scl/fi/ibd4cmixckghx6hhb361c/wikitext-filtered-full.zip?rlkey=q71cebf0k5fvvwhmcntoswzhq&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7cea682ee2d518730f2b216b00.dl-eu.dropboxusercontent.com/cd/0/inline/CzZkFOHnnADPhEXtKAtVphS5JS5H2q1ZUbBHVUI9n6aJI6ozm07RWKGvUUxhnb9CMpRnAbkSqhAoMf8OxXEJOgoGRGSu7rea5L6I9k0fQVoqHAJl1j0AODjDOUcFUAdjX42VbIe9XBZ4tnNdUlfiWTie/file?dl=1# [following]\n",
            "--2025-10-18 00:19:15--  https://uc7cea682ee2d518730f2b216b00.dl-eu.dropboxusercontent.com/cd/0/inline/CzZkFOHnnADPhEXtKAtVphS5JS5H2q1ZUbBHVUI9n6aJI6ozm07RWKGvUUxhnb9CMpRnAbkSqhAoMf8OxXEJOgoGRGSu7rea5L6I9k0fQVoqHAJl1j0AODjDOUcFUAdjX42VbIe9XBZ4tnNdUlfiWTie/file?dl=1\n",
            "Resolving uc7cea682ee2d518730f2b216b00.dl-eu.dropboxusercontent.com (uc7cea682ee2d518730f2b216b00.dl-eu.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc7cea682ee2d518730f2b216b00.dl-eu.dropboxusercontent.com (uc7cea682ee2d518730f2b216b00.dl-eu.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CzYZLYF7SArK5ca4bjK4PBmFJzVBw5FNekpJxTzmZlOBgrgq10cFrykyehG2tDlCC7krzKzmtlHqATPW9n-Yljy-vCYNJxW9DgX-zN2DMFd1jVcBW2FXPSBpHXWHesjBO-pJTtHZna5AvCMuuOS1dJhyLcmSPu2iybndZ_Xj-MaHASxOKHrjn3rrla1tTAinYLWL9A-o_ERCmWuwiVxzz5AMI8u62B7Vl0dkyVB9tc2ccxgnPd0B1D4wvyd_y6eJHBK9sDjc2TBy_G1Omx8g3VNhynG5aNmb36Pn0XcRokUT2HrdpUvoYLp4QO-ucxcJfK5kB9d4c-NNLjWpv7wfQkbSnvClHOXKyzGFyK8CS03O7tsRvTp2N94cVd8NuMaoy3A/file?dl=1 [following]\n",
            "--2025-10-18 00:19:16--  https://uc7cea682ee2d518730f2b216b00.dl-eu.dropboxusercontent.com/cd/0/inline2/CzYZLYF7SArK5ca4bjK4PBmFJzVBw5FNekpJxTzmZlOBgrgq10cFrykyehG2tDlCC7krzKzmtlHqATPW9n-Yljy-vCYNJxW9DgX-zN2DMFd1jVcBW2FXPSBpHXWHesjBO-pJTtHZna5AvCMuuOS1dJhyLcmSPu2iybndZ_Xj-MaHASxOKHrjn3rrla1tTAinYLWL9A-o_ERCmWuwiVxzz5AMI8u62B7Vl0dkyVB9tc2ccxgnPd0B1D4wvyd_y6eJHBK9sDjc2TBy_G1Omx8g3VNhynG5aNmb36Pn0XcRokUT2HrdpUvoYLp4QO-ucxcJfK5kB9d4c-NNLjWpv7wfQkbSnvClHOXKyzGFyK8CS03O7tsRvTp2N94cVd8NuMaoy3A/file?dl=1\n",
            "Reusing existing connection to uc7cea682ee2d518730f2b216b00.dl-eu.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191859755 (183M) [application/binary]\n",
            "Saving to: ‘wikitext-filtered-full.zip’\n",
            "\n",
            "wikitext-filtered-f 100%[===================>] 182.97M  29.3MB/s    in 7.1s    \n",
            "\n",
            "2025-10-18 00:19:25 (25.8 MB/s) - ‘wikitext-filtered-full.zip’ saved [191859755/191859755]\n",
            "\n",
            "--2025-10-18 00:19:25--  https://www.dropbox.com/scl/fi/ek174r3sg7qjx0aa9atop/wikitext-filtered-10k.zip?rlkey=zy6jqxv6qsc16lr9qm3ki9uhf&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc8d2ebb2af79657b7462c16c0a9.dl-eu.dropboxusercontent.com/cd/0/inline/CzYJYsdZnGdyivvDi8WZpcf45Vm7TJYdj4d4ZpdZ7Or4hozLCuNheVhUO1jA11TuYShzOlrPNWNY2U1hIAPudrrnpjdRR3IwLSCd8sj0QuIzaq3MT7nXqyeKCVd0RJGR-jU3PokRv4a4e8UUtxsn08Nt/file?dl=1# [following]\n",
            "--2025-10-18 00:19:25--  https://uc8d2ebb2af79657b7462c16c0a9.dl-eu.dropboxusercontent.com/cd/0/inline/CzYJYsdZnGdyivvDi8WZpcf45Vm7TJYdj4d4ZpdZ7Or4hozLCuNheVhUO1jA11TuYShzOlrPNWNY2U1hIAPudrrnpjdRR3IwLSCd8sj0QuIzaq3MT7nXqyeKCVd0RJGR-jU3PokRv4a4e8UUtxsn08Nt/file?dl=1\n",
            "Resolving uc8d2ebb2af79657b7462c16c0a9.dl-eu.dropboxusercontent.com (uc8d2ebb2af79657b7462c16c0a9.dl-eu.dropboxusercontent.com)... 162.125.4.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc8d2ebb2af79657b7462c16c0a9.dl-eu.dropboxusercontent.com (uc8d2ebb2af79657b7462c16c0a9.dl-eu.dropboxusercontent.com)|162.125.4.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CzbRayn-9dYszcjYqwGX1XB25er1IvHas9Emz6vDy0gViDycLyAM0rxFlvcfmow7nQqGqOaXcSjIcHqnYFeKtG9ecWeBq5j3ThY-smuzgLmY_RY9is85sOX1o583ZiQ8nyNDkGFfnDRs_sVb5uC6bGFKisfIlPa6M88hBMAO587kCWkba3wfH_AUBzZG-zW6ggxod7XlJrVUpG0Wel0BvX82M-YETjQNhKxCK7xx9kM99oLL3TmDG4GiE3lGMKk3LpdhbZBPkU_WpA1fUInhkx0lZDcO1RapJ7x9_j242bazkpecUgvxOrkohUgg9Ebo-mXlePc9ua-8lAKLL6lTJ-DHgcYJgvxGSh_EQYhI8OMxN8HfX5cSSYFf67z1F8PE0qw/file?dl=1 [following]\n",
            "--2025-10-18 00:19:26--  https://uc8d2ebb2af79657b7462c16c0a9.dl-eu.dropboxusercontent.com/cd/0/inline2/CzbRayn-9dYszcjYqwGX1XB25er1IvHas9Emz6vDy0gViDycLyAM0rxFlvcfmow7nQqGqOaXcSjIcHqnYFeKtG9ecWeBq5j3ThY-smuzgLmY_RY9is85sOX1o583ZiQ8nyNDkGFfnDRs_sVb5uC6bGFKisfIlPa6M88hBMAO587kCWkba3wfH_AUBzZG-zW6ggxod7XlJrVUpG0Wel0BvX82M-YETjQNhKxCK7xx9kM99oLL3TmDG4GiE3lGMKk3LpdhbZBPkU_WpA1fUInhkx0lZDcO1RapJ7x9_j242bazkpecUgvxOrkohUgg9Ebo-mXlePc9ua-8lAKLL6lTJ-DHgcYJgvxGSh_EQYhI8OMxN8HfX5cSSYFf67z1F8PE0qw/file?dl=1\n",
            "Reusing existing connection to uc8d2ebb2af79657b7462c16c0a9.dl-eu.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2230160 (2.1M) [application/binary]\n",
            "Saving to: ‘wikitext-filtered-10k.zip’\n",
            "\n",
            "wikitext-filtered-1 100%[===================>]   2.13M  3.17MB/s    in 0.7s    \n",
            "\n",
            "2025-10-18 00:19:27 (3.17 MB/s) - ‘wikitext-filtered-10k.zip’ saved [2230160/2230160]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O wikitext-filtered-full.zip \"https://www.dropbox.com/scl/fi/ibd4cmixckghx6hhb361c/wikitext-filtered-full.zip?rlkey=q71cebf0k5fvvwhmcntoswzhq&dl=1\"\n",
        "!wget -O wikitext-filtered-10k.zip \"https://www.dropbox.com/scl/fi/ek174r3sg7qjx0aa9atop/wikitext-filtered-10k.zip?rlkey=zy6jqxv6qsc16lr9qm3ki9uhf&dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GgyhgwO59KU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c09857e-9342-4c8e-9918-c21b15a2082d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  wikitext-filtered-full.zip\n",
            "replace wikitext-filtered-full/dataset_info.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  wikitext-filtered-10k.zip\n",
            "replace wikitext-filtered-10k/dataset_info.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip wikitext-filtered-full.zip\n",
        "!unzip wikitext-filtered-10k.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gbUoBOOH5_Zy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f73dec-aa89-4602-e9be-d1170d00198e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# datasets package provides dataset tools from hugginface\n",
        "!pip install datasets\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYe6Rq5k6Bgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78031ba7-2883-4278-d784-76de366e4660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wikitext_small: 10000 docs, wikitext_large: 859955 docs\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "def load_dataset():\n",
        "  wikitext_small = \"wikitext-filtered-10k\"\n",
        "  wikitext_large = \"wikitext-filtered-full\"\n",
        "\n",
        "  dataset_small = Dataset.load_from_disk(wikitext_small)\n",
        "  dataset_large = Dataset.load_from_disk(wikitext_large)\n",
        "  print(\"wikitext_small: {} docs, wikitext_large: {} docs\".format(len(dataset_small), len(dataset_large)))\n",
        "  return dataset_small, dataset_large\n",
        "\n",
        "wikitext_small, wikitext_large = load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Dataset\n",
        "Summary statistics"
      ],
      "metadata": {
        "id": "gAUcAjXfgIFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wt = wikitext_small\n",
        "#wt = wikitext_large\n",
        "\n",
        "print('# TYPE OF THE DATASET:', '\\n', type(wt))\n",
        "print(wt, '\\n')\n",
        "print('# ENTRIES LOOK LIKE:')\n",
        "print(wt.features, '\\n', wt[0], '\\n', wt[1], '\\n')\n",
        "\n",
        "print('# DATASET STATISTICS:')\n",
        "print('No. of docs:', len(wt))\n",
        "lengths = [len(doc['text'].split()) for doc in wt]\n",
        "print('Mean doc length:', sum(lengths)/len(lengths), 'words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di-0S5z9d9NE",
        "outputId": "6d449cb6-c57f-4c3f-9fa4-bf14e8a6e72a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# TYPE OF THE DATASET: \n",
            " <class 'datasets.arrow_dataset.Dataset'>\n",
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 10000\n",
            "}) \n",
            "\n",
            "# ENTRIES LOOK LIKE:\n",
            "{'text': Value('string')} \n",
            " {'text': 'Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" .'} \n",
            " {'text': \"The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n .\"} \n",
            "\n",
            "# DATASET STATISTICS:\n",
            "No. of docs: 10000\n",
            "Mean doc length: 115.0954 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 (Train Baselines)\n",
        "\n",
        "---\n",
        "Installing dependancies\n",
        "- gensim - word2vec models\n",
        "- nltk (natural language tool kit) - stopwords removal"
      ],
      "metadata": {
        "id": "fUWa02Kzq3YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk"
      ],
      "metadata": {
        "id": "5490qFGSwT7c",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff59eab6-edbb-47fb-d0bd-fd4d562f89ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_dataset(dataset):\n",
        "    text_col = 'text' if 'text' in dataset.column_names else dataset.column_names[0]\n",
        "    tokenized = []\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        text = dataset[i][text_col]\n",
        "        if not isinstance(text, str):\n",
        "            continue\n",
        "        tokens = [t.lower() for t in text.split() if t.isalpha() and t.lower() not in stop_words]\n",
        "        if tokens:\n",
        "            tokenized.append(tokens)\n",
        "\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "ZMphBXL8nRVE",
        "outputId": "2ec417f9-a2ba-48fa-b484-972c86e4cfa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_small = preprocess_dataset(wikitext_small)\n",
        "tokens_large = preprocess_dataset(wikitext_large)"
      ],
      "metadata": {
        "id": "f8tNPp7Jwb-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_word2vec(tokens, model_name, vector_size=50, window=5, min_count=5, epochs=5):\n",
        "    print(f\"Training {model_name} ...\")\n",
        "    model = Word2Vec(\n",
        "        sentences=tokens,\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        epochs=epochs\n",
        "    )\n",
        "    model.save(f\"{model_name}.model\")\n",
        "    model.wv.save(f\"{model_name}.kv\")\n",
        "    print(f\"{model_name} saved.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "vF6w1eeL6xJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_small = train_word2vec(tokens_small, \"word2vec_small\")\n",
        "model_large = train_word2vec(tokens_large, \"word2vec_large\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMgpuHkuwn0E",
        "outputId": "ba0a5e33-c9cf-4b8c-fe87-1481e105dc4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training word2vec_small ...\n",
            "word2vec_small saved.\n",
            "Training word2vec_large ...\n",
            "word2vec_large saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# writing model to disk\n",
        "model_small.save(\"word2vec_small.model\")\n",
        "model_large.save(\"word2vec_large.model\")"
      ],
      "metadata": {
        "id": "k5bum-3IM5sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_small = Word2Vec.load(\"word2vec_small.model\")\n",
        "model_large = Word2Vec.load(\"word2vec_large.model\")"
      ],
      "metadata": {
        "id": "72LuHLrAN0h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(model_small.wv)\n",
        "print(f\"Vocab size (learned by model):\", vocab_size, '\\n')\n",
        "\n",
        "example_tokens = ['plane', 'car', 'planet', 'nurse', 'city', 'country']\n",
        "for token in example_tokens:\n",
        "    if token in model_small.wv:\n",
        "        print(f\"Top-10 similar to '{token}':\", model_small.wv.most_similar(token, topn=10))\n",
        "    else:\n",
        "        print(f\"'{token}' not in vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H9TVBpxxGTo",
        "outputId": "fba10812-a97c-41ba-c80c-d3d1fabfaf06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size (learned by model): 13838 \n",
            "\n",
            "Top-10 similar to 'plane': [('channel', 0.9976177215576172), ('probe', 0.997555136680603), ('threatened', 0.9975177049636841), ('briefly', 0.9974639415740967), ('recovery', 0.9973675608634949), ('completion', 0.9973640441894531), ('disaster', 0.997336208820343), ('immediate', 0.9971866011619568), ('repairs', 0.9970893859863281), ('arrive', 0.9970740675926208)]\n",
            "Top-10 similar to 'car': [('bring', 0.9986448287963867), ('demon', 0.9985685348510742), ('becomes', 0.9983323812484741), ('possibility', 0.9982467889785767), ('view', 0.9980642795562744), ('reality', 0.9980185627937317), ('leaves', 0.9980082511901855), ('direct', 0.997999906539917), ('fully', 0.9979443550109863), ('owen', 0.9979330897331238)]\n",
            "Top-10 similar to 'planet': [('slightly', 0.9979651570320129), ('indicated', 0.9977326393127441), ('eye', 0.9974536299705505), ('classification', 0.9972626566886902), ('capable', 0.997245728969574), ('engines', 0.9970810413360596), ('offers', 0.9970569610595703), ('observed', 0.9970449805259705), ('moons', 0.9970031976699829), ('approached', 0.9969111084938049)]\n",
            "Top-10 similar to 'nurse': [('tawny', 0.9985595941543579), ('shark', 0.9983565211296082), ('attributed', 0.9981611967086792), ('effect', 0.9980685710906982), ('green', 0.9979763627052307), ('oxygen', 0.9979245662689209), ('painting', 0.9978524446487427), ('ring', 0.9978417754173279), ('vulnerable', 0.997806191444397), ('stela', 0.997787356376648)]\n",
            "Top-10 similar to 'city': [('state', 0.920772910118103), ('county', 0.9116392731666565), ('national', 0.9073209762573242), ('town', 0.8833724856376648), ('southern', 0.8818227648735046), ('moved', 0.8810088634490967), ('highway', 0.8779670000076294), ('northern', 0.8759586215019226), ('route', 0.8752599954605103), ('headquartered', 0.874614953994751)]\n",
            "Top-10 similar to 'country': [('countries', 0.9848932027816772), ('belgium', 0.9840196371078491), ('usa', 0.9834767580032349), ('network', 0.9832971096038818), ('toured', 0.9821997284889221), ('official', 0.982041597366333), ('riaa', 0.9810083508491516), ('adult', 0.980902373790741), ('industry', 0.9804458022117615), ('shipments', 0.9797428846359253)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "tPze_vBWUket"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "-AvWc8V9GKDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mag(v):\n",
        "  s = sum((e*e) for e in v)\n",
        "  s = math.sqrt(s)\n",
        "  return s\n",
        "\n",
        "def cosineSimilarity(v1, v2):\n",
        "  dotProd = np.dot(v1, v2)\n",
        "  cos = dotProd/(mag(v1)*mag(v2))\n",
        "  return cos"
      ],
      "metadata": {
        "id": "_VJaT2I9_Ren"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "planeIndex = model_small.wv.key_to_index['plane']\n",
        "carIndex = model_small.wv.key_to_index['car']\n",
        "planetIndex = model_small.wv.key_to_index['planet']\n",
        "sunIndex = model_small.wv.key_to_index['sun']\n",
        "passengerIndex = model_small.wv.key_to_index['passenger']"
      ],
      "metadata": {
        "id": "JDg-ByjIGc0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v1 = model_large.wv['planet']\n",
        "v2 = model_large.wv['sun']\n",
        "\n",
        "cosineSimilarity(v1, v2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--jP1qvpGNme",
        "outputId": "3d01e15f-be4a-4103-9b87-3b89cd3597c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6323668823823705"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4"
      ],
      "metadata": {
        "id": "e8MAgY7DUS80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O wordsim353.zip \"www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip\"\n",
        "!unzip wordsim353.zip -d wordsim353"
      ],
      "metadata": {
        "id": "0ZiC8uMRUScW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "368e3b23-4f54-48a8-d35a-89f1d79a8dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-18 00:23:24--  http://www.gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
            "Resolving www.gabrilovich.com (www.gabrilovich.com)... 173.236.137.139\n",
            "Connecting to www.gabrilovich.com (www.gabrilovich.com)|173.236.137.139|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://gabrilovich.com/resources/data/wordsim353/wordsim353.zip [following]\n",
            "--2025-10-18 00:23:24--  https://gabrilovich.com/resources/data/wordsim353/wordsim353.zip\n",
            "Resolving gabrilovich.com (gabrilovich.com)... 173.236.137.139\n",
            "Connecting to gabrilovich.com (gabrilovich.com)|173.236.137.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23257 (23K) [application/zip]\n",
            "Saving to: ‘wordsim353.zip’\n",
            "\n",
            "wordsim353.zip      100%[===================>]  22.71K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-10-18 00:23:24 (650 KB/s) - ‘wordsim353.zip’ saved [23257/23257]\n",
            "\n",
            "Archive:  wordsim353.zip\n",
            "replace wordsim353/combined.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import spearmanr"
      ],
      "metadata": {
        "id": "00CYbfprfOOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(\"wordsim353/combined.csv\")\n",
        "print(\"Loaded: wordsim353/combined.csv | shape:\", df.shape)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03qp2mamfgDI",
        "outputId": "5e7c2408-3ac5-4093-8745-4da90f37ed63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded: wordsim353/combined.csv | shape: (353, 3)\n",
            "     Word 1    Word 2  Human (mean)\n",
            "0      love       sex          6.77\n",
            "1     tiger       cat          7.35\n",
            "2     tiger     tiger         10.00\n",
            "3      book     paper          7.46\n",
            "4  computer  keyboard          7.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eDkTUzGi0UEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare vocabularies\n",
        "wordsim_vocab = set(df[\"Word 1\"].str.lower()).union(set(df[\"Word 2\"].str.lower()))\n",
        "\n",
        "smallModel_vocab = set(model_small.wv.key_to_index)\n",
        "largeModel_vocab = set(model_large.wv.key_to_index)\n",
        "\n",
        "sharedVocab = wordsim_vocab.intersection(largeModel_vocab)\n",
        "OOV = wordsim_vocab.difference(largeModel_vocab)\n",
        "\n",
        "print(OOV) # should be empty"
      ],
      "metadata": {
        "id": "oaCQ4x-u0UQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sim_or_nan_strict(model, a, b):\n",
        "    if a not in model.wv.key_to_index or b not in model.wv.key_to_index:\n",
        "        return np.nan\n",
        "    v1 = model.wv[a]\n",
        "    v2 = model.wv[b]\n",
        "    return float(cosineSimilarity(v1, v2))\n"
      ],
      "metadata": {
        "id": "iWQbEDDdmGXV"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ws_spearman(model, df):\n",
        "    sims = df.apply(lambda r: sim_or_nan_strict(model, r['Word 1'], r['Word 2']), axis=1)\n",
        "    gold = df['Human (mean)']\n",
        "    valid = sims.notna()\n",
        "\n",
        "\n",
        "    if not valid.any():\n",
        "        return np.nan, np.nan, 0, len(df), len(df)\n",
        "\n",
        "    rho, p = spearmanr(sims[valid], gold[valid])\n",
        "\n",
        "    used = int(valid.sum())\n",
        "    total = int(len(df))\n",
        "    n_oov = total - used\n",
        "    return float(rho), float(p), used, total, n_oov\n",
        "\n",
        "\n",
        "# alternative simpler implamentation\n",
        "def applySpearman_to_WordSim(model, df):\n",
        "  df = df.copy()\n",
        "  df[\"Word 1\"] = df[\"Word 1\"].str.lower()\n",
        "  df[\"Word 2\"] = df[\"Word 2\"].str.lower()\n",
        "\n",
        "  similarities = df.apply(\n",
        "      lambda r: model.wv.similarity(r[\"Word 1\"], r[\"Word 2\"]),\n",
        "      axis = 1\n",
        "  )\n",
        "\n",
        "  human_standard = df[\"Human (mean)\"].astype(float)\n",
        "  rho, p_value = spearmanr(similarities, human_standard)\n",
        "\n",
        "  return float(rho), float(p_value)"
      ],
      "metadata": {
        "id": "Ri3mC4PdouDa"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for label, m in [(\"wikitext_small\", model_small), (\"wikitext_large\", model_large)]:\n",
        "    rho, p, used, total, n_oov = ws_spearman(m, df)\n",
        "    print(f\"[{label}] Spearman ρ = {rho:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4biy_LT_o2-L",
        "outputId": "6a17bf53-518c-45ad-a821-79998fd23086"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[wikitext_small] Spearman ρ = 0.0779\n",
            "[wikitext_large] Spearman ρ = 0.6342\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "athnlp-lab3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}