{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 (Enviroment Setup)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Importing the Datasets\n",
        "wikitext (*full* ) - 859955 docs\n",
        "\n",
        "wikitext (*small* ) - 10000 docs"
      ],
      "metadata": {
        "id": "I02HtuGWggmv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sQaGMPUu5vQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30aa073d-8d6f-4a80-cecd-75a5ff321ba4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-17 23:04:24--  https://www.dropbox.com/scl/fi/ibd4cmixckghx6hhb361c/wikitext-filtered-full.zip?rlkey=q71cebf0k5fvvwhmcntoswzhq&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:601c:18::a27d:612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com/cd/0/inline/CzaM-GcUFl25wnxZLuuZEjPle2s8cbOBXGUdauuTx4bVyCkbHbKJUfztLG3QrW5FRDVIUwJSHM88MN9fg7ti5opxoNh6rR4SszpW_0stmIH7BuoPSPh_MbPtg1K0JHjBqlKuDbSk_AEyCcpoIy8yHpzC/file?dl=1# [following]\n",
            "--2025-10-17 23:04:25--  https://uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com/cd/0/inline/CzaM-GcUFl25wnxZLuuZEjPle2s8cbOBXGUdauuTx4bVyCkbHbKJUfztLG3QrW5FRDVIUwJSHM88MN9fg7ti5opxoNh6rR4SszpW_0stmIH7BuoPSPh_MbPtg1K0JHjBqlKuDbSk_AEyCcpoIy8yHpzC/file?dl=1\n",
            "Resolving uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com (uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com)... 162.125.4.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com (uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com)|162.125.4.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CzatVmYTxzr1lHUtVLUl8t3YmJ_QgdqUpsnLuYJzaEu8Y7CJKowOgXgkHR2Uij936jWMPoS9zA_SiUgAhfuEbrBtxftsQs-izEu1DZH1llm4KT6R5-Z-M42z9CTHO6TWTsAOkkv9Rzjkm7qe_oKxkkhlA4hQtAeUD9eXe25mytE9iAuW7QCbwxsUYDzPqplxrjRzYA9klgmZeDWOz2bWS81G3uyPx6fnOxgAMww4cFsdtcyoa3ul6U5hcOWRPObvcAF_9Tx3Ab8qMVpERF13zXch326tm61AgheKoR30nH0GTziLMcX0gnFCpRDddA1khTTdZa1H8glrTdDXNxw2ouudRs_6oJ6ga3TKHDMD5cFbc0vmIdTDkEE4wyx4tcGNc6o/file?dl=1 [following]\n",
            "--2025-10-17 23:04:26--  https://uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com/cd/0/inline2/CzatVmYTxzr1lHUtVLUl8t3YmJ_QgdqUpsnLuYJzaEu8Y7CJKowOgXgkHR2Uij936jWMPoS9zA_SiUgAhfuEbrBtxftsQs-izEu1DZH1llm4KT6R5-Z-M42z9CTHO6TWTsAOkkv9Rzjkm7qe_oKxkkhlA4hQtAeUD9eXe25mytE9iAuW7QCbwxsUYDzPqplxrjRzYA9klgmZeDWOz2bWS81G3uyPx6fnOxgAMww4cFsdtcyoa3ul6U5hcOWRPObvcAF_9Tx3Ab8qMVpERF13zXch326tm61AgheKoR30nH0GTziLMcX0gnFCpRDddA1khTTdZa1H8glrTdDXNxw2ouudRs_6oJ6ga3TKHDMD5cFbc0vmIdTDkEE4wyx4tcGNc6o/file?dl=1\n",
            "Reusing existing connection to uceea71edf314d203d1ecea54905.dl-eu.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 191859755 (183M) [application/binary]\n",
            "Saving to: ‘wikitext-filtered-full.zip’\n",
            "\n",
            "wikitext-filtered-f 100%[===================>] 182.97M  35.6MB/s    in 5.9s    \n",
            "\n",
            "2025-10-17 23:04:33 (31.2 MB/s) - ‘wikitext-filtered-full.zip’ saved [191859755/191859755]\n",
            "\n",
            "--2025-10-17 23:04:33--  https://www.dropbox.com/scl/fi/ek174r3sg7qjx0aa9atop/wikitext-filtered-10k.zip?rlkey=zy6jqxv6qsc16lr9qm3ki9uhf&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:601c:18::a27d:612\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com/cd/0/inline/CzYAB524zKhymHaDklfBe-FLMMv_dS40O34JQIV9NZ7QjEiEgOKYc8qsXvuBD-aAc-FBysPShI3smpoX3eGr-zZajiktWftS5UI6AcxKvOuOh2yDjvzw3H7an6gm-eeLGGTA_uJuC2Sa5WjKft2Z_dP1/file?dl=1# [following]\n",
            "--2025-10-17 23:04:34--  https://uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com/cd/0/inline/CzYAB524zKhymHaDklfBe-FLMMv_dS40O34JQIV9NZ7QjEiEgOKYc8qsXvuBD-aAc-FBysPShI3smpoX3eGr-zZajiktWftS5UI6AcxKvOuOh2yDjvzw3H7an6gm-eeLGGTA_uJuC2Sa5WjKft2Z_dP1/file?dl=1\n",
            "Resolving uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com (uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com)... 162.125.3.15, 2620:100:601c:15::a27d:60f\n",
            "Connecting to uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com (uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CzaS5GNzConvHg-wNOYT7cIC8k0MkKqhVQmCDfwFuyzcXA9XiUez0aUuvE4FZiq5PIItku6u-NHK_xx04JtsgWUPSEMLI96NDLGXVYiR3_HCzxgM82wjfvcfL3_fG9gdi8IfmIEeSHcT94I6OLAB5ohB1gmiKRDuRp5Y82C1F-XkPmwG0OftHZ7H7K4OEhA0ojN9EFd1ofbty2sP3ZxgiLsXAwcf2AdgsBStlELtsvkrOkfx1Exwcx695fS9DmgwjV_cXI_gz-0uPnJKMIk5SS3zUU3DdvhWC9HsOFOyKip0LcjuTtPwvYKFcIllb0oM0211p9rni8DLUPav9827XCwlyZp6ZgJBZVW2dWIONxO-P050sX2WotqpwE2Hcspa_b0/file?dl=1 [following]\n",
            "--2025-10-17 23:04:34--  https://uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com/cd/0/inline2/CzaS5GNzConvHg-wNOYT7cIC8k0MkKqhVQmCDfwFuyzcXA9XiUez0aUuvE4FZiq5PIItku6u-NHK_xx04JtsgWUPSEMLI96NDLGXVYiR3_HCzxgM82wjfvcfL3_fG9gdi8IfmIEeSHcT94I6OLAB5ohB1gmiKRDuRp5Y82C1F-XkPmwG0OftHZ7H7K4OEhA0ojN9EFd1ofbty2sP3ZxgiLsXAwcf2AdgsBStlELtsvkrOkfx1Exwcx695fS9DmgwjV_cXI_gz-0uPnJKMIk5SS3zUU3DdvhWC9HsOFOyKip0LcjuTtPwvYKFcIllb0oM0211p9rni8DLUPav9827XCwlyZp6ZgJBZVW2dWIONxO-P050sX2WotqpwE2Hcspa_b0/file?dl=1\n",
            "Reusing existing connection to uc607e19353764d48e1cb6a98cb0.dl-eu.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2230160 (2.1M) [application/binary]\n",
            "Saving to: ‘wikitext-filtered-10k.zip’\n",
            "\n",
            "wikitext-filtered-1 100%[===================>]   2.13M  2.62MB/s    in 0.8s    \n",
            "\n",
            "2025-10-17 23:04:36 (2.62 MB/s) - ‘wikitext-filtered-10k.zip’ saved [2230160/2230160]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O wikitext-filtered-full.zip \"https://www.dropbox.com/scl/fi/ibd4cmixckghx6hhb361c/wikitext-filtered-full.zip?rlkey=q71cebf0k5fvvwhmcntoswzhq&dl=1\"\n",
        "!wget -O wikitext-filtered-10k.zip \"https://www.dropbox.com/scl/fi/ek174r3sg7qjx0aa9atop/wikitext-filtered-10k.zip?rlkey=zy6jqxv6qsc16lr9qm3ki9uhf&dl=1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0GgyhgwO59KU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "264bd5cd-f122-4120-acfc-fe172a3ad732",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  wikitext-filtered-full.zip\n",
            "replace wikitext-filtered-full/dataset_info.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: Archive:  wikitext-filtered-10k.zip\n",
            "replace wikitext-filtered-10k/dataset_info.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip wikitext-filtered-full.zip\n",
        "!unzip wikitext-filtered-10k.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gbUoBOOH5_Zy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0ee1bc-2b37-46ee-ef5f-4387be6a0f13",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# datasets package provides dataset tools from hugginface\n",
        "!pip install datasets\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aYe6Rq5k6Bgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc791372-8bad-41d4-9c6d-3debdf2fccd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wikitext_small: 10000 docs, wikitext_large: 859955 docs\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "def load_dataset():\n",
        "  wikitext_small = \"wikitext-filtered-10k\"\n",
        "  wikitext_large = \"wikitext-filtered-full\"\n",
        "\n",
        "  dataset_small = Dataset.load_from_disk(wikitext_small)\n",
        "  dataset_large = Dataset.load_from_disk(wikitext_large)\n",
        "  print(\"wikitext_small: {} docs, wikitext_large: {} docs\".format(len(dataset_small), len(dataset_large)))\n",
        "  return dataset_small, dataset_large\n",
        "\n",
        "wikitext_small, wikitext_large = load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Dataset\n",
        "Summary statistics"
      ],
      "metadata": {
        "id": "gAUcAjXfgIFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wt = wikitext_small\n",
        "#wt = wikitext_large\n",
        "\n",
        "print('# TYPE OF THE DATASET:', '\\n', type(wt))\n",
        "print(wt, '\\n')\n",
        "print('# ENTRIES LOOK LIKE:')\n",
        "print(wt.features, '\\n', wt[0], '\\n', wt[1], '\\n')\n",
        "\n",
        "print('# DATASET STATISTICS:')\n",
        "print('No. of docs:', len(wt))\n",
        "lengths = [len(doc['text'].split()) for doc in wt]\n",
        "print('Mean doc length:', sum(lengths)/len(lengths), 'words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di-0S5z9d9NE",
        "outputId": "568031af-214b-4ce3-adc0-ff14d197cb68"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# TYPE OF THE DATASET: \n",
            " <class 'datasets.arrow_dataset.Dataset'>\n",
            "Dataset({\n",
            "    features: ['text'],\n",
            "    num_rows: 10000\n",
            "}) \n",
            "\n",
            "# ENTRIES LOOK LIKE:\n",
            "{'text': Value('string')} \n",
            " {'text': 'Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" .'} \n",
            " {'text': \"The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n .\"} \n",
            "\n",
            "# DATASET STATISTICS:\n",
            "No. of docs: 10000\n",
            "Mean doc length: 115.0954 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 (Train Baselines)\n",
        "\n",
        "---\n",
        "Installing dependancies\n",
        "- gensim - word2vec models\n",
        "- nltk (natural language tool kit) - stopwords removal"
      ],
      "metadata": {
        "id": "fUWa02Kzq3YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim nltk\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk"
      ],
      "metadata": {
        "id": "5490qFGSwT7c",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7b6ce2-72d6-41eb-c3f6-97780cf066be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_dataset(dataset):\n",
        "    text_col = 'text' if 'text' in dataset.column_names else dataset.column_names[0]\n",
        "    tokenized = []\n",
        "\n",
        "    for i in range(len(dataset)):\n",
        "        text = dataset[i][text_col]\n",
        "        if not isinstance(text, str):\n",
        "            continue\n",
        "        tokens = [t.lower() for t in text.split() if t.isalpha() and t.lower() not in stop_words]\n",
        "        if tokens:\n",
        "            tokenized.append(tokens)\n",
        "\n",
        "    return tokenized"
      ],
      "metadata": {
        "id": "ZMphBXL8nRVE",
        "outputId": "c04cbeca-0eb1-40d8-c29f-9572d17230bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_small = preprocess_dataset(wikitext_small)\n",
        "tokens_large = preprocess_dataset(wikitext_large)"
      ],
      "metadata": {
        "id": "f8tNPp7Jwb-i"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_word2vec(tokens, model_name, vector_size=50, window=5, min_count=5, epochs=5):\n",
        "    print(f\"Training {model_name} ...\")\n",
        "    model = Word2Vec(\n",
        "        sentences=tokens,\n",
        "        vector_size=vector_size,\n",
        "        window=window,\n",
        "        min_count=min_count,\n",
        "        epochs=epochs\n",
        "    )\n",
        "    model.save(f\"{model_name}.model\")\n",
        "    model.wv.save(f\"{model_name}.kv\")\n",
        "    print(f\"{model_name} saved.\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "vF6w1eeL6xJW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_small = train_word2vec(tokens_small, \"word2vec_small\")\n",
        "model_large = train_word2vec(tokens_large, \"word2vec_large\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMgpuHkuwn0E",
        "outputId": "ba0a5e33-c9cf-4b8c-fe87-1481e105dc4d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training word2vec_small ...\n",
            "word2vec_small saved.\n",
            "Training word2vec_large ...\n",
            "word2vec_large saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# writing model to disk\n",
        "model_small.save(\"word2vec_small.model\")\n",
        "model_large.save(\"word2vec_large.model\")"
      ],
      "metadata": {
        "id": "k5bum-3IM5sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_small = Word2Vec.load(\"word2vec_small.model\")\n",
        "model_large = Word2Vec.load(\"word2vec_large.model\")"
      ],
      "metadata": {
        "id": "72LuHLrAN0h_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(model_small.wv)\n",
        "print(f\"Vocab size (learned by model):\", vocab_size, '\\n')\n",
        "\n",
        "example_tokens = ['plane', 'car', 'planet', 'nurse', 'city', 'country']\n",
        "for token in example_tokens:\n",
        "    if token in model_small.wv:\n",
        "        print(f\"Top-10 similar to '{token}':\", model_small.wv.most_similar(token, topn=10))\n",
        "    else:\n",
        "        print(f\"'{token}' not in vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H9TVBpxxGTo",
        "outputId": "b757f037-2177-492e-d789-d1e1eb98b544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size (learned by model): 162898 \n",
            "\n",
            "Top-10 similar to 'plane': [('airplane', 0.7798449397087097), ('planes', 0.7587494850158691), ('helicopter', 0.7551898956298828), ('flight', 0.7514647245407104), ('wreckage', 0.7437148690223694), ('airliner', 0.7287061214447021), ('takeoff', 0.7201843857765198), ('saucer', 0.7154824733734131), ('cockpit', 0.7090662717819214), ('amana', 0.701010525226593)]\n",
            "Top-10 similar to 'car': [('cars', 0.8536964058876038), ('truck', 0.8398393988609314), ('motorcycle', 0.8149896860122681), ('lorry', 0.801920473575592), ('suv', 0.7947259545326233), ('vehicle', 0.7883414030075073), ('driver', 0.7828571200370789), ('cab', 0.7772312164306641), ('taxi', 0.7629276514053345), ('tires', 0.7254185080528259)]\n",
            "Top-10 similar to 'planet': [('earth', 0.8504156470298767), ('galaxy', 0.835283100605011), ('pluto', 0.7836962938308716), ('planets', 0.7812984585762024), ('asteroid', 0.7718139886856079), ('uranus', 0.7695909738540649), ('universe', 0.7661747932434082), ('jupiter', 0.7612099051475525), ('neptune', 0.7575763463973999), ('supernova', 0.7567441463470459)]\n",
            "Top-10 similar to 'nurse': [('surgeon', 0.7863967418670654), ('matron', 0.7805107235908508), ('nanny', 0.7734796404838562), ('therapist', 0.7701335549354553), ('intern', 0.7607481479644775), ('housekeeper', 0.7600995898246765), ('schwester', 0.753458559513092), ('prostitute', 0.744369626045227), ('dentist', 0.7410475611686707), ('midwife', 0.7406240105628967)]\n",
            "Top-10 similar to 'city': [('town', 0.7932745814323425), ('suburbs', 0.7008221745491028), ('cities', 0.6958555579185486), ('downtown', 0.6860154271125793), ('minneapolis', 0.6775717735290527), ('towns', 0.6729977130889893), ('neighborhood', 0.6519626379013062), ('waterfront', 0.6501344442367554), ('metropolitan', 0.6485058665275574), ('capital', 0.6436895728111267)]\n",
            "Top-10 similar to 'country': [('nation', 0.7754936218261719), ('world', 0.6323301792144775), ('europe', 0.6227652430534363), ('countries', 0.6060277223587036), ('european', 0.5809457898139954), ('india', 0.5773490071296692), ('africans', 0.5741065144538879), ('britain', 0.5740565657615662), ('blacks', 0.569057285785675), ('state', 0.562152087688446)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3"
      ],
      "metadata": {
        "id": "tPze_vBWUket"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "-AvWc8V9GKDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mag(v):\n",
        "  s = sum((e*e) for e in v)\n",
        "  s = math.sqrt(s)\n",
        "  return s\n",
        "\n",
        "def cosineSimilarity(v1, v2):\n",
        "  dotProd = np.dot(v1, v2)\n",
        "  cos = dotProd/(mag(v1)*mag(v2))\n",
        "  return cos"
      ],
      "metadata": {
        "id": "_VJaT2I9_Ren"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "planeIndex = model_small.wv.key_to_index['plane']\n",
        "carIndex = model_small.wv.key_to_index['car']\n",
        "planetIndex = model_small.wv.key_to_index['planet']\n",
        "sunIndex = model_small.wv.key_to_index['sun']\n",
        "passengerIndex = model_small.wv.key_to_index['passenger']"
      ],
      "metadata": {
        "id": "JDg-ByjIGc0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v1 = model_large.wv['planet']\n",
        "v2 = model_large.wv['sun']\n",
        "\n",
        "cosineSimilarity(v1, v2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--jP1qvpGNme",
        "outputId": "c0b78992-1d92-4e1b-98b7-4952af65d080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6306870286899026"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4"
      ],
      "metadata": {
        "id": "e8MAgY7DUS80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O combined.csv \"https://raw.githubusercontent.com/kavgan/nlp-text-mining-working-examples/master/wordSimilarity/resources/wordsim353/combined.csv\"\n"
      ],
      "metadata": {
        "id": "0ZiC8uMRUScW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas scipy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import spearmanr"
      ],
      "metadata": {
        "id": "00CYbfprfOOd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O combined.zip \"https://raw.githubusercontent.com/kavgan/nlp-text-mining-working-examples/master/wordSimilarity/resources/wordsim353/combined.csv\"\n",
        "\n",
        "!unzip -o combined.zip\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"combined.csv\")  # columns: 'Word 1', 'Word 2', 'Human (mean)'\n",
        "print(\"Loaded: wordsim353/combined.csv | shape:\", df.shape)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "03qp2mamfgDI",
        "outputId": "94822d65-1423-425b-86d1-06ed38fcba25"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  combined.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of combined.zip or\n",
            "        combined.zip.zip, and cannot find combined.zip.ZIP, period.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "EmptyDataError",
          "evalue": "No columns to parse from file",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3226201610.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"combined.csv\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# columns: 'Word 1', 'Word 2', 'Human (mean)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded: wordsim353/combined.csv | shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "athnlp-lab3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}